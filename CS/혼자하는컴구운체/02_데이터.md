# 0과 1로 숫자를 표현하는 방법

컴퓨터는 0과 1로 모든 정보를 표현하고, 0과 1로 표현된 정보만을 이해할 수 있음.

컴퓨터가 표현하는 **정보 단위**를 학습하고, **0과 1만**으로 숫자를 표현하는 방법을 배워본다.

## 정보 단위

0과 1을 나타내는 가장 작은 정보 단위를 **비트**라고 한다.

비트는 간단하게 이해하자면, ON/OFF와 원리가 같다.

1비트는 2가지를 표현하고,

2비트는 4가지, 3비트는 8가지, 4비트는 16가지 …

비트는 즉, 2의 거듭제곱만큼 표현 가능해진다.

우리가 실행하는 모든 프로그램은,

수 십만, 수 백만 개 이상의 0과 1로 이루어져 있다.

그러나 우리는 프로그램 크기를 말할 때, 비트 단위로만 설명하지 않는다.

큰 단위를 말하고 싶을 때 편의성을 위해

**byte, kB, GB, TB** 등으로 나타낼 수 있겠다.

| **단위** | **크기** |
| --- | --- |
| 1 byte | 8 bit |
| 1 KB | 10^3 byte = 8 x 10^3 bit |
| 1 MB | 10^3 kB = 10^6 byte = 8 x 10^6 bit |
| 1 GB | 10^3 MB = 10^6 kB = 10^9 byte = 8 x 10^9 bit |
| 1 TB | 10^3 GB = 10^6MB = 10^9 kB = 10^12 byte = 8 x 10^12 bit |
| 1 PB(페타바이트) | 이하 생략… |
| 1 EB(엑사바이트) |  |
| 1 ZB(제타바이트) |  |
| 1 YB(요타바이트) |  |
| 1 RB(론나바이트) |  |
| 1 QB(퀘타바이트) |  |

*비트와 바이트의 중간 단위로 니블(nibble) 이라는 게 존재한다. → 4비트 = 1니블*

그리고,

1kB = 1,024byte, 1MB = 1,024kB 이런 식으로 표현하는 것은 잘못된 습관이다!

*~~대학에서 이렇게 외웠는데…~~*

1,024개 묶어 표현한 단위는 **KiB, MiB, GiB, TiB** 이다.

### 참고 : 워드

중요한 정보 단위 중 워드라는 단위도 있다.

CPU가 한 번에 처리할 수 있는 데이터 크기를 의미한다.

만약 CPU가 한 번에 16비트를 처리할 수 있다면, 1워드는 16비트를 의미한다.

절반 크기는 **하프 워드**, 1배 크기를 **풀 워드**, 2배 크기를 **더블 워드** 라고 부른다.

대부분의 컴퓨터 워드 크기는 32비트 or 64비트이다.

## 이진법

수학에서 0과 1만으로 모든 숫자를 표현하는 방법.

보통 우리가 사용하는 숫자 표현 방법은 **십진법**이다. (0~9)

이진법으로 표현한 수 : **이진수**, 십진법으로 표현한 수 : **십진수**

컴퓨터에게 어떤 숫자를 알려주려면 이진수로 변경하여 알려줘야 한다.

*ex)* 10 → 1010 (2), 8 → 1000 (2) ….

여기에서 뒤에 쓴 *‘(2)’* 는 이진법을 표현하기 위한 것이다. → 수학 방식

혹은 이진 수 앞에 *‘0b’* 를 붙이기도 한다. → 코드 방식

### 이진수의 음수 표현

컴퓨터는 0과 1만 이해할 수 있기 때문에 마이너스 부호를 사용하지 않고 0과 1만으로 음수를 표현해야 한다.

가장 널리 사용되는 방법은, **2의 보수**를 구해 음수로 간주하는 방법이다.

<aside>

**2의 보수** : 어떤 수를 그보다 큰 2^n에서 뺀 값

</aside>

*ex)* 11(2) 의 2의 보수는 이 값보다 큰 2^n → 100(2)

이것에서 11(2)을 뺀 **01(2)**이 **11(2)**의 보수가 된다.

*어려워 보인다..*

그러나 단순하게!

모든 값을 다 뒤집고, 1을 더하면 된다. 11(2) → 00(2) → **01(2)** *~~참 쉽죠?~~*

모든 이진수의 **0과 1을 뒤집은 수**를 **1의 보수**라고 하고,

**거기에 1을 더한 값**을 **2의 보수**라고 한다.

그렇다면 1011(2)의 음수를 구해보면 어떨까?

마찬가지로 2의 보수를 구하면 1011(2) → 0100(2) → **0101(2)** 이다.

그렇다면 이 값의 음수를 또 구하면 원래대로 돌아오겠네? *→ -(-A) = A 원리*

그렇습니다. 0101(2) → 1010(2) → **1011(2)**

그런데 0101(2)는 숫자 5라고 말할 수도 있는데, 이게 음수인지 어떻게 판별할까?

실제로 이진수만 봐서는 음수인지 양수인지 구분하기 어렵다.

그래서 컴퓨터 내부에서 어떤 수를 다룰 때는 양수인지 음수인지 구분하기 위해 **플래그**를 사용한다!

**플래그는 쉽게 말해 부가 정보이다.**

*~~→ 이 내용은 04장에서 자세히 다룰 예정이라 되어 있다!~~*

### 참고 : 2의 보수 표현의 한계

2의 보수는 음수를 표현하기 위해 가장 널리 사용되는 방식이지만, 완벽한 방식은 아니다!

아래의 예를 확인해보자.

1. 0000(2) → 1111(2) → **10000(2)**
2. 1000(2) → 0111(2) → **1000(2)** *~~띠용~~*

1번과 같을 때는 앞에 1을 버려서 해결한다. *→ 사실 0의 음수는 표현할 필요가 없어서 그런 게 아닐까?*

하지만 2번과 같은 경우는 2^n 의 음수를 표현하기 힘들다.

그러므로, n비트로는 -2^n과 2^n을 동시에 표현할 수 없다.

## 십육진법

이진법은 0과 1만으로 모든 숫자를 표현하다 보니 숫자의 길이가 너무 길어진다는 단점이 있다.

그래서 데이터를 표현할 때, 십육진법도 자주 사용된다.

<aside>

**십육진법** : 15를 넘어가는 시점에 자리 올림을 하는 숫자 표현 방식

*ex)* 10 → A(16), 11 → B(16), 12 → C(16) …

</aside>

이진수에 비해 더 적은 자릿수로 더 많은 정보를 표현할 수 있겠다!

16진법을 표현하는 방법은 뒤에 작게 *‘(16)’* 을 쓰거나, → 수학 방식

숫자 앞에 *‘0x’*를 붙인다. → 코드 방식

### 십육진수를 이진수로 변환하기

십육진수 숫자 하나 당 4비트를 나타내므로, 각 자리를 잘라서 4비트로 표현해서 붙이면 된다!

*ex)* 1A2B (16) → (0001 (2)) (1010 (2)) (0010 (2)) (1011 (2)) → **0001101000101011 (2)**

### 이진수를 십육진수로 변환하기

이진수 자리를 4개씩 끊어서 16진수로 표현하고 붙이면 된다!

ex) 11010101 (2) → (1101 (2)) (0101 (2)) → (D (16)) (5 (16)) → **D5 (16)**

*십육진수는 프로그래밍할 때 이진수와 더불어 자주 사용되므로 기억해 두는 것이 좋다!*

# 0과 1로 문자를 표현하는 방법

컴퓨터는 0과 1만 이해할 수 있다고 했는데, 필자가 입력한 문자를 어떻게 이해하고 모니터에 출력하는 것일까?

컴퓨터가 문자를 이해하고 표현하는 다양한 방법에 대해 알아보자.

## 문자 집합과 인코딩

### 문자 집합

컴퓨터가 인식하고 표현할 수 있는 문자의 모음.

컴퓨터는 문자 집합에 속해 있는 문자를 이해할 수 있고, 문자 집합에 속해 있찌 않은 문자는 이해할 수 없다.

*ex)* 문자 집합이 {a, b, c, d, e} 라면, 컴퓨터는 이 다섯 개 외 문자를 이해할 수 없다.

### 문자 인코딩 (Character encoding)

문자 집합에 속한 문자라고 해서 컴퓨터가 그대로 이해할 수 있는 건 아니다!

문자를 0과 1로 변환해야 컴퓨터가 이해할 수 있으므로,

이 변환 과정을 **문자 인코딩**이라고 한다.

같은 문자 집합에 대해서 다양한 인코딩 방법이 있을 수도 있다.

### 문자 디코딩 (Character decoding)

인코딩의 반대 과정, 0과 1로 이루어진 문자 코드를 사람이 이해할 수 있는 문자로 변환하는 과정이다.

## 아스키 코드

### 아스키 (ASCII)

**초창기 문자 집합** 중 하나.

영어 알파벳과 아라비아 숫자, 일부 특수 문자를 포함한다.

아스키 문자 집합에 속한 문자들은 7비트로 표현된다. → 2^7 = 128개 문자.

ㄱ러나 실제로 아스키 문자를 나타내기 위해 8비트를 사용한다.

*8비트 중 1비트는 패리티 비트(parity bit) 라고, 오류 검출을 위해 사용되는 비트이기 때문에 실질적으로 사용되는 비트는 7비트라고 하는 것이다.*

아스키 문자에 대응된 고유한 수를 **아스키 코드**라고 한다.

*ex)*

‘A’ 는 아스키 코드로 65 → 1000001 (2) 로 인코딩 된다.

‘a’ 는 아스키 코드로 97 → 1100001 (2) 로 인코딩 된다.

*그래서 C언어 할 때 `print("%c", 65);` 라고 하면 A로 출력했던 거 같다.*

*문자 인코딩에서 글자에 부여된 고유한 값을 **코드 포인트** 라고 한다.*

그러나 치명적인 단점, 한글을 표현할 수 없다!

한글뿐만 아니라 아스키 문자 집합 외의 문자, 특수문자도 표현할 수 없게 된다.

이후에 8비트의 **확장 아스키**가 등장하기도 했지만, 그럼에도 256개일뿐 턱없이 부족하다.

## EUC-KR

알파벳을 쭉 이어 쓰면 단어가 되는 영어와 달리,

한글은 각 음절 **하나하나가 초성, 중성, 종성의 조합으로 이루어져 있다.**

<aside>

**완성형 인코딩**

초성, 중성, 종성의 조합으로 이루어진 완성된 하나의 글자에 고유한 코드를 부여하는 인코딩 방식이다.

ex) 가 → 1, 나 → 2 …

</aside>

<aside>

**조합형 인코딩**

초성을 위한 비트열, 중성을 위한 비트열, 종성을 위한 비트열을 할당하여 그것들의 조합으로 하나의 글자 코드를 완성하는 인코딩 방식

초성, 중성, 종성에 해당하는 코드를 합하여 하나의 글자 코드를 만드는 인코딩 방식이다.

</aside>

**EUC-KR** 은 KS X 1001, KS X 1003 이라는 문자 집합을 기반으로 하는 대표적인 완성형 인코딩 방식이다.

즉, 초성, 중성, 종성이 모두 결합된 한글 단어에 2바이트 크기에 코드를 부여한다.

한글 한 글자에 2바이트 코드가 부여되고, EUC-KR로 인코딩된 한글 한 글자를 표현하려면 16비트가 필요하다. 이것은 네 자리 십육진수로 표현 가능하다.

EUC-KR 인코딩 방식으로 2,350개 정도의 한글을 표현할 수 있지만, 정의되지 않은 한글 ‘꿹’, ‘쀓’ 등은 표현할 수 없다.

→ 그래서 웹사이트나 학교, 은행 등에서 피해를 입기도 했다.

그나마 뒤에 **CP949** 라는 인코딩 방식이 생겨났지만, 여전히 문제가 발생하였다.

## 유니코드와 UTF-8

EUC-KR 인코딩 덕분에 한국어를 코드로 표현할 수 있었지만, 한계가 발생함.

그리고 언어 별로 인코딩을 나라마다 해야 한다면, 다국어를 지원하는 프로그램을 만들 때, 각 나라 언어의 인코딩을 모두 알아야 하는 번거로움이 생긴다.

그래서 등장한 것이 **유니코드** 이다.

유니코드는 EUC-KR보다 훨씬 다양한 한글을 포함하며 대부분 나라의 문자, 특수문자, 화살표, 이모티콩까지 코드로 표현할 수 있는 통일된 문자 집합이다.

유니코드도 역시 완성형 인코딩 방식처럼 각 문자마다 고유한 값이 부여된다.

(아스키, EUC-KR 과 동일하다.)

*간혹 앞에 ‘U+’ 가 숫자 앞에 붙어있기도 한데, 이것도 16진수를 표현하는 방법이라 생각하면 된다.*

그러나 유니코드는 글자에 부여된 값 자체를 인코딩된 값으로 삼지 않고!

다양한 방법으로 인코딩한다.

*UTF-8, UTF-16, UTF-32 …*

### UTF-8

가장 대중적인 유니코드 인코딩 방식이다.

**1바이트부터 4바이트 까지**의 인코딩 결과를 만들어 낸다.

유니코드 문자에 부여된 값의 범위에 따라 몇 바이트가 될지 결정된다고 보면 된다.

*1바이트 → 0~007F (16)*

*2바이트 → 0080 (16) ~ 07FF (16)*

*3바이트 → 0800 (16) ~ FFFF (16)*

*4바이트 → 10000 (16) ~ 10FFFF (16)*

*ex) ‘한글’* 같은 경우, 두 글자 ‘한’, ‘글’ 이 3바이트 내에 코드가 존재하므로, 인코딩하면 3바이트로 표현될 것이다.